{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/ACP_DeepSRC/blob/main/ACP_DeepSRC_Kfold_Stratified_SKv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tcc6qiBvH3A"
      },
      "outputs": [],
      "source": [
        "import sys, os, re, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "## Models\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical # for keras > 2.0\n",
        "\n",
        "## Perfmetrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import auc, average_precision_score, precision_recall_curve, roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO4GsEwi6UZp"
      },
      "outputs": [],
      "source": [
        "def SRC_Pred(y_latent_pred):\n",
        "    y_pred=[]\n",
        "    for i in range(y_latent_pred.shape[1]):\n",
        "\n",
        "        if (LA.norm(y_latent_pred[i][y_train==1])>= LA.norm(y_latent_pred[i][y_train==0])):\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "\n",
        "    return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydW7vLSY4v6Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d7VDajGydiX"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def Deep_SRC(input_shape=840, LV=600):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\n",
        "    enc_l1 = Dense(100, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\n",
        "\n",
        "    enc_l2 = Dense(100, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\n",
        "\n",
        "    enc_l3 = Dense(100, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\n",
        "\n",
        "    enc_l4 = Dense(100, activation='relu', name='encoder_layer4')(enc_l3)\n",
        "    enc_l4 = BatchNormalization()(enc_l4)\n",
        "    enc_l4 = Dropout(rate = 0.3)(enc_l4)\n",
        "\n",
        "    encoder_output = Dense(LV, activation='softmax', name='encoder_output')(enc_l4)\n",
        "\n",
        "    # # Classifier Network\n",
        "    #class_output = Dense(2, activation='softmax', name='class_layer1')(encoder_output)\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l4)\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[encoder_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\n",
        "                  loss={'encoder_output': 'categorical_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  loss_weights={'encoder_output': 0.001, 'decoder_output': 0.999},\n",
        "                  metrics=[metrics.categorical_accuracy])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ams3z_J54Dq1"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def Deep_VAE_SRC(input_shape =840, LV=600):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\n",
        "    enc_l1 = Dense(100, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\n",
        "\n",
        "    enc_l2 = Dense(100, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\n",
        "\n",
        "    enc_l3 = Dense(100, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\n",
        "\n",
        "    # enc_l4 = Dense(100, activation='relu', name='encoder_layer4')(enc_l3)\n",
        "    # enc_l4 = BatchNormalization()(enc_l4)\n",
        "    # enc_l4 = Dropout(rate = 0.3)(enc_l4)\n",
        "\n",
        "    #encoder_output = Dense(LV, activation='softmax', name='encoder_output')(enc_l4)\n",
        "    z_mean = Dense(LV, name=\"z_mean\")(enc_l3)\n",
        "    z_log_var = Dense(LV, name=\"z_log_var\")(enc_l3)\n",
        "    encoder_output = Sampling()([z_mean, z_log_var])\n",
        "    encoder_output = Dense(LV, activation='relu', name='encoder_output')(encoder_output)\n",
        "\n",
        "    # # Classifier Network\n",
        "    #class_output = Dense(2, activation='softmax', name='class_layer1')(encoder_output)\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l4)\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[encoder_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\n",
        "                  loss={'encoder_output': 'categorical_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  loss_weights={'encoder_output': 0.5, 'decoder_output': 0.5},\n",
        "                  metrics=[metrics.categorical_accuracy])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmKm-WgAYTva"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def Deep_SRC_Decoder(input_shape =600, num_features=2400):\n",
        "    # Encoder Network\n",
        "    dec_input = Input(shape=(input_shape,), name='decoder_input')\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(dec_input)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(num_features, activation='sigmoid', name='decoder_output')(dec_l4)\n",
        "\n",
        "    model = Model(inputs=[dec_input], outputs=[decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\n",
        "                  loss={'decoder_output': 'mean_squared_error'},\n",
        "                  metrics=[metrics.mean_squared_error])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M679bns3u7p7"
      },
      "outputs": [],
      "source": [
        "## Define CKSAAP feature-extraction function\n",
        "def minSequenceLength(fastas):\n",
        "\tminLen = 10000\n",
        "\tfor i in fastas:\n",
        "\t\tif minLen > len(i[1]):\n",
        "\t\t\tminLen = len(i[1])\n",
        "\treturn minLen\n",
        "\n",
        "def CKSAAP(fastas, gap=5, **kw):\n",
        "\tif gap < 0:\n",
        "\t\tprint('Error: the gap should be equal or greater than zero' + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tif minSequenceLength(fastas) < gap+2:\n",
        "\t\tprint('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tAA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\tencodings = []\n",
        "\taaPairs = []\n",
        "\tfor aa1 in AA:\n",
        "\t\tfor aa2 in AA:\n",
        "\t\t\taaPairs.append(aa1 + aa2)\n",
        "\theader = ['#']\n",
        "\tfor g in range(gap+1):\n",
        "\t\tfor aa in aaPairs:\n",
        "\t\t\theader.append(aa + '.gap' + str(g))\n",
        "\tencodings.append(header)\n",
        "\tfor i in fastas:\n",
        "\t\tname, sequence = i[0], i[1]\n",
        "\t\tcode = [name]\n",
        "\t\tfor g in range(gap+1):\n",
        "\t\t\tmyDict = {}\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tmyDict[pair] = 0\n",
        "\t\t\tsum = 0\n",
        "\t\t\tfor index1 in range(len(sequence)):\n",
        "\t\t\t\tindex2 = index1 + g + 1\n",
        "\t\t\t\tif index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\n",
        "\t\t\t\t\tmyDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
        "\t\t\t\t\tsum = sum + 1\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tcode.append(myDict[pair] / sum)\n",
        "\t\tencodings.append(code)\n",
        "\treturn encodings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_seq_data(data_path,label):\n",
        "  dataset = pd.read_csv(data_path,names=None,index_col=0, header=None)\n",
        "  seq = []\n",
        "  sample_count = 0\n",
        "\n",
        "  for row in dataset.iterrows():\n",
        "    if(row[0]!='>'):\n",
        "      sample_count = sample_count +1\n",
        "      array = [label, row[0]]\n",
        "      name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n",
        "      seq.append([name, sequence])\n",
        "\n",
        "  print('# of ' + label + ' samples',sample_count)\n",
        "  return seq"
      ],
      "metadata": {
        "id": "WWQAg0KY5O2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_feature_acp240_740(path = r\"acp240.txt\"):\n",
        "    # path = r\"acp740.txt\"\n",
        "    # path = r\"acp240.txt\"\n",
        "    new_list=[]\n",
        "    seq_list=[]\n",
        "    label = []\n",
        "    lis = []\n",
        "    interaction_pair = {}\n",
        "    RNA_seq_dict = {}\n",
        "    protein_seq_dict = {}\n",
        "    protein_index = 0\n",
        "    with open(path, 'r') as fp:\n",
        "        for line in fp:\n",
        "            if line[0] == '>':\n",
        "                values = line[1:].strip().split('|')\n",
        "                label_temp = values[1]\n",
        "                proteinName = values[0]\n",
        "                proteinName_1=proteinName.split(\"_\")\n",
        "                new_list.append(proteinName_1[0])\n",
        "    #             print(new_list)\n",
        "\n",
        "                if label_temp == '1':\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    label.append(0)\n",
        "            else:\n",
        "                seq = line[:-1]\n",
        "                seq_list.append(seq)\n",
        "        for i, item in enumerate(new_list):\n",
        "            lis.append([item, seq_list[i]])\n",
        "\n",
        "    return lis\n"
      ],
      "metadata": {
        "id": "B0nnBtvW5XaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Convert_Seq2CKSAAP(train_seq, gap=8):\n",
        "  cksaapfea = []\n",
        "  seq_label = []\n",
        "  for sseq in train_seq:\n",
        "    temp= CKSAAP([sseq], gap=gap)\n",
        "    cksaapfea.append(temp[1][1:])\n",
        "    seq_label.append(sseq[0])\n",
        "\n",
        "  x = np.array(cksaapfea)\n",
        "  y = np.array(seq_label)\n",
        "  y[y=='ACP']=0\n",
        "  y[y=='non-ACP']=1\n",
        "  y[y=='POS']=0\n",
        "  y[y=='NEG']=1\n",
        "  y = to_categorical(y)\n",
        "  print('num pos:', sum(y[:,0]==1), 'num neg:', sum(y[:,0]==0))\n",
        "  return x,y\n",
        "\n",
        "def minSequenceLength(fastas):\n",
        "    minLen = 10000\n",
        "    for i in fastas:\n",
        "        if minLen > len(i[1]):\n",
        "            minLen = len(i[1])\n",
        "    return minLen\n",
        "\n",
        "def CKSAAP(fastas, gap=5, **kw):\n",
        "    if gap < 0:\n",
        "        print('Error: the gap should be equal or greater than zero' + '\\n\\n')\n",
        "        return 0\n",
        "\n",
        "    if minSequenceLength(fastas) < gap+2:\n",
        "        print('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n' + 'Current sequence length ='  + str(minSequenceLength(fastas)) + '\\n\\n')\n",
        "        return 0\n",
        "\n",
        "    AA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "    encodings = []\n",
        "    aaPairs = []\n",
        "    for aa1 in AA:\n",
        "        for aa2 in AA:\n",
        "            aaPairs.append(aa1 + aa2)\n",
        "    header = ['#']\n",
        "    for g in range(gap+1):\n",
        "        for aa in aaPairs:\n",
        "            header.append(aa + '.gap' + str(g))\n",
        "    encodings.append(header)\n",
        "    for i in fastas:\n",
        "        name, sequence = i[0], i[1]\n",
        "        code = [name]\n",
        "        for g in range(gap+1):\n",
        "            myDict = {}\n",
        "            for pair in aaPairs:\n",
        "                myDict[pair] = 0\n",
        "            sum = 0\n",
        "            for index1 in range(len(sequence)):\n",
        "                index2 = index1 + g + 1\n",
        "                if index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\n",
        "                    myDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
        "                    sum = sum + 1\n",
        "            for pair in aaPairs:\n",
        "                code.append(myDict[pair] / sum)\n",
        "        encodings.append(code)\n",
        "    return encodings"
      ],
      "metadata": {
        "id": "pyHjVPn96GCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Loading and pre-processing of dataset\n",
        "pos_all_seq_path = 'https://raw.githubusercontent.com/Shujaat123/ACP_LSE/main/dataset_acp_JTB_2014/1-s2.0-S0022519313004190-mmc1.txt'\n",
        "neg_all_seq_path = 'https://raw.githubusercontent.com/Shujaat123/ACP_LSE/main/dataset_acp_JTB_2014/1-s2.0-S0022519313004190-mmc2.txt'\n",
        "\n",
        "!pip install wget\n",
        "import wget\n",
        "dataset_path='https://raw.githubusercontent.com/haichengyi/ACP-DL/master/acp740.txt'\n",
        "wget.download(dataset_path, 'acp740.txt')\n",
        "\n",
        "dataset_path='https://raw.githubusercontent.com/haichengyi/ACP-DL/master/acp240.txt'\n",
        "wget.download(dataset_path, 'acp240.txt')\n",
        "\n",
        "!pip install mrmr_selection\n",
        "from mrmr import mrmr_classif\n",
        "\n",
        "pos_all_seq = load_seq_data(pos_all_seq_path,'POS')\n",
        "neg_all_seq = load_seq_data(neg_all_seq_path,'NEG')\n",
        "\n",
        "ALL_seq344 = pos_all_seq + neg_all_seq\n",
        "\n",
        "print(len(pos_all_seq), len(neg_all_seq), len(ALL_seq344))\n",
        "\n",
        "ALL_seq240=prepare_feature_acp240_740(path = r\"acp240.txt\")\n",
        "ALL_seq740=prepare_feature_acp240_740(path = r\"acp740.txt\")\n",
        "\n",
        "print(len(ALL_seq240))\n",
        "print(len(ALL_seq344))\n",
        "print(len(ALL_seq740))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9TUbxDK5EX3",
        "outputId": "3ee5bfe9-c58f-41c3-bf18-51f358b63713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: mrmr_selection in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: category-encoders in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (3.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (4.66.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.4.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.11.4)\n",
            "Requirement already satisfied: polars>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (0.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2024.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders->mrmr_selection) (0.14.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders->mrmr_selection) (0.5.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mrmr_selection) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mrmr_selection) (2.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category-encoders->mrmr_selection) (1.16.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category-encoders->mrmr_selection) (24.0)\n",
            "# of POS samples 138\n",
            "# of NEG samples 206\n",
            "138 206 344\n",
            "240\n",
            "344\n",
            "740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yZMgda6FGMm"
      },
      "outputs": [],
      "source": [
        "# train_set = pd.read_csv(\"https://raw.githubusercontent.com/Shujaat123/AFP-SRC/master/data/train1.csv\")\n",
        "# test_set = pd.read_csv(\"https://raw.githubusercontent.com/Shujaat123/AFP-SRC/master/data/test1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua12za40GXE6"
      },
      "outputs": [],
      "source": [
        "# # from keras.utils.np_utils import to_categorical\n",
        "# X_train = train_set.iloc[:, 1:].to_numpy()\n",
        "# y_train = np.asarray(train_set.CLASS)\n",
        "# y_train[y_train=='AFP']=1\n",
        "# y_train[y_train=='NON_AFP']=0\n",
        "# y_train = to_categorical(y_train)\n",
        "\n",
        "# X_test = test_set.iloc[:, 1:].to_numpy()\n",
        "# y_test = np.asarray(test_set.CLASS)\n",
        "# y_test[y_test=='AFP']=1\n",
        "# y_test[y_test=='NON_AFP']=0\n",
        "# y_test = to_categorical(y_test)\n",
        "\n",
        "# print('X_train.shape:',X_train.shape,'X_test.shape:',X_test.shape)\n",
        "# print('y_train.shape:',y_train.shape,'y_test.shape:',y_test.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLhyrZXbvitu"
      },
      "outputs": [],
      "source": [
        "def sample_one_hot_encoder(label):\n",
        "  ntrain = len(y_train)\n",
        "  onehot_encoded = list()\n",
        "  for value in range(ntrain):\n",
        "    letter = [0 for _ in range(ntrain)]\n",
        "    letter[value] = 1\n",
        "    onehot_encoded.append(letter)\n",
        "\n",
        "  return np.array(onehot_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HbJtcuKxABo"
      },
      "outputs": [],
      "source": [
        "# y_train_latent = sample_one_hot_encoder(y_train)\n",
        "# y_train_latent.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkGc8BX_32Wq"
      },
      "outputs": [],
      "source": [
        "import numpy.linalg as LA\n",
        "def SRC_Pred(y_latent_pred):\n",
        "    y_pred=[]\n",
        "    for i in range(y_latent_pred.shape[0]):\n",
        "\n",
        "        if (LA.norm(y_latent_pred[i][y_train.argmax(axis=1)==1])>= LA.norm(y_latent_pred[i][y_train.argmax(axis=1)==0])):\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "\n",
        "    return to_categorical(np.array(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL2lNsS1zqmf"
      },
      "outputs": [],
      "source": [
        "# y_train_latent_pred.shape, y_test_latent_pred.shape\n",
        "# y_train_pred = SRC_Pred_Dict_Recon(y_train, y_train_latent_pred, X_train, X_train)\n",
        "# y_test_pred = SRC_Pred_Dict_Recon(y_test, y_test_latent_pred, X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEwgli58CaQr",
        "outputId": "8380d5c0-434c-43b9-aecc-03e3545f8074"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01275478, 0.03467109, 0.25618664, 0.69638749])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def custom_softmax(x):\n",
        "  exp_x = np.exp(x - np.max(x))\n",
        "  return exp_x / np.sum(exp_x)\n",
        "x = np.array([1, 2, 4, 5])\n",
        "custom_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fjpqSTKGWBB"
      },
      "outputs": [],
      "source": [
        "def SRC_Pred_Dict_Recon(y_train, y_latent_pred, X_train, X_test):\n",
        "  npos = (y_train.argmax(axis=1)==0).sum()\n",
        "  nneg = len(y_train) - npos\n",
        "  y_latent_pred_pos = []\n",
        "  y_latent_pred_neg = []\n",
        "  for i in range(y_latent_pred.shape[0]):\n",
        "     y_latent_pred_pos.append(np.concatenate((np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==0], np.repeat(0,nneg)), axis=0))\n",
        "     y_latent_pred_neg.append(np.concatenate((np.repeat(0,npos), np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1]), axis=0))\n",
        "\n",
        "  X_pred_pos = np.matmul(np.array(y_latent_pred_pos), X_train)\n",
        "  X_pred_neg = np.matmul(np.array(y_latent_pred_neg), X_train)\n",
        "\n",
        "  y_pred=[]\n",
        "  for i in range(X_test.shape[0]):\n",
        "    pos_err = np.square(X_test[i] - X_pred_pos).sum()\n",
        "    neg_err = np.square(X_test[i] - X_pred_neg).sum()\n",
        "    if pos_err <= neg_err:\n",
        "      y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(1)\n",
        "\n",
        "  return to_categorical(np.array(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sZOgQnV2Mfx"
      },
      "outputs": [],
      "source": [
        "# y_latent_pred = np.random.rand(len(y_train),len(y_train))\n",
        "# y_pred = SRC_Pred(y_latent_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeh-L3Jp7icZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ6O1q4Y8VFh"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkZEW7TIJ_QM"
      },
      "outputs": [],
      "source": [
        "def SRC_Pred_Dec_Recon(y_train, y_latent_pred, model, X_test, argmax=False):\n",
        "  npos = (y_train.argmax(axis=1)==0).sum()\n",
        "  nneg = len(y_train) - npos\n",
        "  y_latent_pred_pos = []\n",
        "  y_latent_pred_neg = []\n",
        "  for i in range(y_latent_pred.shape[0]):\n",
        "    if argmax:\n",
        "      tmp_y_latent_pred_pos = np.repeat(0, len(y_latent_pred[i]))\n",
        "      tmp_y_latent_pred_pos[y_latent_pred[i][y_train==0].argmax(axis=1)] = 1\n",
        "      tmp_y_latent_pred_neg = np.repeat(0, len(y_latent_pred[i]))\n",
        "      tmp_y_latent_pred_neg[y_latent_pred[i][y_train==1].argmax(axis=1)] = 1\n",
        "      y_latent_pred_pos.append(tmp_y_latent_pred_pos)\n",
        "      y_latent_pred_neg.append(tmp_y_latent_pred_neg)\n",
        "    else:\n",
        "     y_latent_pred_pos.append(np.concatenate((np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==0], np.repeat(0,nneg)), axis=0))\n",
        "     y_latent_pred_neg.append(np.concatenate((np.repeat(0,npos), np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1]), axis=0))\n",
        "\n",
        "  y_latent_pred_pos = np.array(y_latent_pred_pos)\n",
        "  y_latent_pred_neg = np.array(y_latent_pred_neg)\n",
        "  X_pred_pos = model_dec.predict(y_latent_pred_pos)\n",
        "  X_pred_neg = model_dec.predict(y_latent_pred_neg)\n",
        "\n",
        "  # print('y_latent_pred_neg.shape:',y_latent_pred_neg.shape)\n",
        "\n",
        "  y_pred=[]\n",
        "  # print('X_test.shape[0]:',X_test.shape[0])\n",
        "  for i in range(X_test.shape[0]):\n",
        "    # print('X_test.shape:',X_test.shape)\n",
        "    # print('X_pred_pos.shape:',X_pred_pos.shape)\n",
        "    # print('X_pred_neg.shape:',X_pred_neg.shape)\n",
        "    pos_err = np.square(X_test[i] - X_pred_pos[i]).sum()\n",
        "    neg_err = np.square(X_test[i] - X_pred_neg[i]).sum()\n",
        "    if pos_err <= neg_err:\n",
        "      y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(1)\n",
        "  return np.array(y_pred)\n",
        "  # return to_categorical(np.array(y_pred))\n",
        "\n",
        "# print('X_train.shape:',X_train.shape)\n",
        "# print(10*'*')\n",
        "# y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz3Q97vju6Ho"
      },
      "outputs": [],
      "source": [
        "def yoden_index(y, y_pred):\n",
        "  epsilon = 1e-30\n",
        "  tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
        "  j = (tp/(tp + fn + epsilon)) + (tn/(tn+fp + epsilon)) - 1\n",
        "  return j\n",
        "\n",
        "def pmeasure(y, y_pred):\n",
        "    epsilon = 1e-30\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
        "    sensitivity = tp / (tp + fn + epsilon)\n",
        "    specificity = tn / (tn + fp + epsilon)\n",
        "    f1score = (2 * tp) / (2 * tp + fp + fn + epsilon)\n",
        "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDk5IAHJs3f4"
      },
      "outputs": [],
      "source": [
        "def Calculate_Stats(y_actual,y_pred):\n",
        "  acc = accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  sen = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Sensitivity']\n",
        "  spe = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Specificity']\n",
        "  f1 = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['F1-Score']\n",
        "  mcc = matthews_corrcoef(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  bacc = balanced_accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  yi = yoden_index(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  #auc = roc_auc_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "\n",
        "  #pre, rec, _ = precision_recall_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\n",
        "  #fpr, tpr, _ = roc_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\n",
        "  #auroc = auc(fpr, tpr)\n",
        "  #aupr = auc(rec, pre)\n",
        "\n",
        "  return acc, sen, spe, f1, mcc, bacc, yi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_dimensions(pred_array):\n",
        "  if(len(pred_array.shape)==1):\n",
        "    pred_array = np.expand_dims(pred_array,axis=1)\n",
        "    pred_array = np.concatenate((pred_array,np.abs(1-pred_array)),axis=1)\n",
        "  return pred_array"
      ],
      "metadata": {
        "id": "jwiXey9pHrd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaCNG--RtbIx"
      },
      "outputs": [],
      "source": [
        "def Show_Statistics(msg,mean_Stats, sd_Stats, sigfig):\n",
        "  print(msg.upper())\n",
        "  print(70*'-')\n",
        "  print('Accuracy:{} + {}'          .format(round(mean_Stats[0],sigfig), round(sd_Stats[0],sigfig)))\n",
        "  print('Sensitivity:{} + {} '      .format(round(mean_Stats[1],sigfig), round(sd_Stats[1],sigfig)))\n",
        "  print('Specificity:{} + {}'       .format(round(mean_Stats[2],sigfig), round(sd_Stats[2],sigfig)))\n",
        "  print('F1-Score:{} + {}'          .format(round(mean_Stats[3],sigfig), round(sd_Stats[3],sigfig)))\n",
        "  print('MCC:{} + {}'               .format(round(mean_Stats[4],sigfig), round(sd_Stats[4],sigfig)))\n",
        "  print('Balance Accuracy:{} + {}'  .format(round(mean_Stats[5],sigfig), round(sd_Stats[5],sigfig)))\n",
        "  print('Youden-Index:{} + {}'      .format(round(mean_Stats[6],sigfig), round(sd_Stats[6],sigfig)))\n",
        "  print(70*'-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwjTfOFszbhv",
        "outputId": "f83ccd5b-f9b1-4951-c490-3fcdcb86bdfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num pos: 138 num neg: 206\n",
            "X_train.shape: (172, 2400) X_test.shape: (172, 2400)\n",
            "y_train.shape: (172, 2) y_test.shape: (172, 2)\n",
            "y_test_latent_pred.shape: (172, 172)\n",
            "y_test.shape: (172, 2) y_test_src_pred.shape (172, 2)\n",
            "By Reconstruction (Model)\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "y_train.shape: (172, 2) y_train_src_rec_pred.shape (172, 2)\n",
            "train_acc_by_Norm: 0.5988372093023255, test_acc_by_Norm: 0.5988372093023255\n",
            "train_acc_by_Dict: 0.9302325581395349, test_acc_by_Dict: 0.9069767441860465\n",
            "train_acc_by_Recon: 0.5988372093023255, test_acc_by_Recon: 0.5988372093023255\n",
            "21.622006001870805 22.377929749618986\n",
            "NORM TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.156\n",
            "Sensitivity:1.0 + 0.009 \n",
            "Specificity:0.0 + 0.403\n",
            "F1-Score:0.749 + 0.092\n",
            "MCC:0.0 + 0.404\n",
            "Balance Accuracy:0.5 + 0.197\n",
            "Youden-Index:0.0 + 0.394\n",
            "----------------------------------------------------------------------\n",
            "NORM TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.145\n",
            "Sensitivity:1.0 + 0.018 \n",
            "Specificity:0.0 + 0.389\n",
            "F1-Score:0.749 + 0.083\n",
            "MCC:0.0 + 0.38\n",
            "Balance Accuracy:0.5 + 0.186\n",
            "Youden-Index:0.0 + 0.371\n",
            "----------------------------------------------------------------------\n",
            "DICT TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.93 + 0.156\n",
            "Sensitivity:0.981 + 0.009 \n",
            "Specificity:0.855 + 0.403\n",
            "F1-Score:0.944 + 0.092\n",
            "MCC:0.856 + 0.404\n",
            "Balance Accuracy:0.918 + 0.197\n",
            "Youden-Index:0.836 + 0.394\n",
            "----------------------------------------------------------------------\n",
            "DICT TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.907 + 0.145\n",
            "Sensitivity:0.961 + 0.018 \n",
            "Specificity:0.826 + 0.389\n",
            "F1-Score:0.925 + 0.083\n",
            "MCC:0.807 + 0.38\n",
            "Balance Accuracy:0.894 + 0.186\n",
            "Youden-Index:0.787 + 0.371\n",
            "----------------------------------------------------------------------\n",
            "REC TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.156\n",
            "Sensitivity:1.0 + 0.009 \n",
            "Specificity:0.0 + 0.403\n",
            "F1-Score:0.749 + 0.092\n",
            "MCC:0.0 + 0.404\n",
            "Balance Accuracy:0.5 + 0.197\n",
            "Youden-Index:0.0 + 0.394\n",
            "----------------------------------------------------------------------\n",
            "REC TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.145\n",
            "Sensitivity:1.0 + 0.018 \n",
            "Specificity:0.0 + 0.389\n",
            "F1-Score:0.749 + 0.083\n",
            "MCC:0.0 + 0.38\n",
            "Balance Accuracy:0.5 + 0.186\n",
            "Youden-Index:0.0 + 0.371\n",
            "----------------------------------------------------------------------\n",
            "X_train.shape: (172, 2400) X_test.shape: (172, 2400)\n",
            "y_train.shape: (172, 2) y_test.shape: (172, 2)\n",
            "y_test_latent_pred.shape: (172, 172)\n",
            "y_test.shape: (172, 2) y_test_src_pred.shape (172, 2)\n",
            "By Reconstruction (Model)\n",
            "6/6 [==============================] - 0s 5ms/step\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "6/6 [==============================] - 0s 3ms/step\n",
            "y_train.shape: (172, 2) y_train_src_rec_pred.shape (172, 2)\n",
            "train_acc_by_Norm: 0.5988372093023255, test_acc_by_Norm: 0.5988372093023255\n",
            "train_acc_by_Dict: 0.9651162790697675, test_acc_by_Dict: 0.8546511627906976\n",
            "train_acc_by_Recon: 0.4011627906976744, test_acc_by_Recon: 0.4011627906976744\n",
            "16.109757602346992 22.378889947189933\n",
            "NORM TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.234\n",
            "Sensitivity:1.0 + 0.469 \n",
            "Specificity:0.0 + 0.455\n",
            "F1-Score:0.749 + 0.416\n",
            "MCC:0.0 + 0.437\n",
            "Balance Accuracy:0.5 + 0.216\n",
            "Youden-Index:0.0 + 0.433\n",
            "----------------------------------------------------------------------\n",
            "NORM TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.186\n",
            "Sensitivity:1.0 + 0.463 \n",
            "Specificity:0.0 + 0.419\n",
            "F1-Score:0.749 + 0.39\n",
            "MCC:0.0 + 0.33\n",
            "Balance Accuracy:0.5 + 0.155\n",
            "Youden-Index:0.0 + 0.31\n",
            "----------------------------------------------------------------------\n",
            "DICT TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.965 + 0.234\n",
            "Sensitivity:0.99 + 0.469 \n",
            "Specificity:0.928 + 0.455\n",
            "F1-Score:0.971 + 0.416\n",
            "MCC:0.928 + 0.437\n",
            "Balance Accuracy:0.959 + 0.216\n",
            "Youden-Index:0.918 + 0.433\n",
            "----------------------------------------------------------------------\n",
            "DICT TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.855 + 0.186\n",
            "Sensitivity:0.961 + 0.463 \n",
            "Specificity:0.696 + 0.419\n",
            "F1-Score:0.888 + 0.39\n",
            "MCC:0.701 + 0.33\n",
            "Balance Accuracy:0.828 + 0.155\n",
            "Youden-Index:0.657 + 0.31\n",
            "----------------------------------------------------------------------\n",
            "REC TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.401 + 0.234\n",
            "Sensitivity:0.0 + 0.469 \n",
            "Specificity:1.0 + 0.455\n",
            "F1-Score:0.0 + 0.416\n",
            "MCC:0.0 + 0.437\n",
            "Balance Accuracy:0.5 + 0.216\n",
            "Youden-Index:0.0 + 0.433\n",
            "----------------------------------------------------------------------\n",
            "REC TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.401 + 0.186\n",
            "Sensitivity:0.0 + 0.463 \n",
            "Specificity:1.0 + 0.419\n",
            "F1-Score:0.0 + 0.39\n",
            "MCC:0.0 + 0.33\n",
            "Balance Accuracy:0.5 + 0.155\n",
            "Youden-Index:0.0 + 0.31\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "[DataX, LabelY] = Convert_Seq2CKSAAP(ALL_seq344, gap=5)\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=2,shuffle=True, random_state=25)\n",
        "loop_ind = 0\n",
        "for train_index, test_index in kf.split(DataX,LabelY.argmax(axis=1)):\n",
        "  loop_ind = loop_ind + 1\n",
        "  X_train, X_test = DataX[train_index], DataX[test_index]\n",
        "  y_train, y_test = LabelY[train_index], LabelY[test_index]\n",
        "  print('X_train.shape:',X_train.shape,'X_test.shape:',X_test.shape)\n",
        "  print('y_train.shape:',y_train.shape,'y_test.shape:',y_test.shape)\n",
        "\n",
        "  #############################################\n",
        "  y_train_latent = sample_one_hot_encoder(y_train)\n",
        "  y_train_latent.shape[1]\n",
        "  #############################################\n",
        "\n",
        "  model = Deep_SRC(input_shape = X_train.shape[1], LV=len(y_train))\n",
        "  #model = Deep_VAE_SRC(input_shape = X_train.shape[1], LV=len(y_train))\n",
        "  es = EarlyStopping(monitor='encoder_output_categorical_accuracy', mode='max', verbose=0, patience=100)\n",
        "  checkpoint = ModelCheckpoint('models\\\\model-best.h5',\n",
        "                              verbose=0, monitor='encoder_output_categorical_accuracy',save_best_only=True, mode='auto')\n",
        "\n",
        "  history = model.fit({'enc_input': X_train},\n",
        "                      {'encoder_output': y_train_latent, 'decoder_output': X_train},\n",
        "                      # validation_data = ({'enc_input': X_val},\n",
        "                      # {'class_output': y_val, 'decoder_output': X_val}),\n",
        "                      epochs=5000, batch_size=420, callbacks=[checkpoint, es], verbose=0)\n",
        "\n",
        "  y_train_latent_pred, X_train_pred = model.predict(X_train,batch_size=540, verbose=0)\n",
        "  #y_val_latent_pred, X_val_pred = model.predict(X_val,batch_size=540, verbose=0)\n",
        "  y_test_latent_pred, X_test_pred = model.predict(X_test,batch_size=540, verbose=0)\n",
        "\n",
        "  print('y_test_latent_pred.shape:',y_test_latent_pred.shape)\n",
        "  #############################################################\n",
        "  #############################################################\n",
        "  #del model\n",
        "  #model = load_model('models\\\\model-best.h5')\n",
        "\n",
        "  model_dec = Deep_SRC_Decoder(input_shape = len(y_train), num_features=X_train.shape[1])\n",
        "\n",
        "  # es = EarlyStopping(monitor='loss', mode='min', verbose=0, patience=100)\n",
        "  # checkpoint = ModelCheckpoint('models\\\\model_dec-best.h5',\n",
        "  #                              verbose=0, monitor='loss',save_best_only=True, mode='auto')\n",
        "\n",
        "  # history = model_dec.fit({'decoder_input': y_train_latent},\n",
        "  #                     {'decoder_output': X_train},\n",
        "  #                     # validation_data = ({'enc_input': X_val},\n",
        "  #                     # {'class_output': y_val, 'decoder_output': X_val}),\n",
        "  #                     epochs=5000, batch_size=600, callbacks=[checkpoint, es], verbose=0)\n",
        "  # del model_dec\n",
        "  # model_dec = load_model('models\\\\model_dec-best.h5')\n",
        "\n",
        "  for i in range(1,12):\n",
        "      model_dec.layers[i].set_weights(model.layers[13+i].get_weights())\n",
        "\n",
        "  ## By Norm\n",
        "  y_train_src_pred = SRC_Pred(y_train_latent_pred)\n",
        "  y_test_src_pred = SRC_Pred(y_test_latent_pred)\n",
        "  train_src_pred_acc = accuracy_score(y_train, y_train_src_pred)\n",
        "  print('y_test.shape:',y_test.shape,'y_test_src_pred.shape',y_test_src_pred.shape)\n",
        "  test_src_pred_acc = accuracy_score(y_test, y_test_src_pred)\n",
        "\n",
        "  ## By Dictionary (Train Dataset)\n",
        "  y_train_src_dict_pred = SRC_Pred_Dict_Recon(y_train, y_train_latent_pred, X_train, X_train)\n",
        "  y_test_src_dict_pred = SRC_Pred_Dict_Recon(y_train, y_test_latent_pred, X_train, X_test)\n",
        "  train_src_dict_acc = accuracy_score(y_train, y_train_src_dict_pred)\n",
        "  test_src_dict_acc = accuracy_score(y_test, y_test_src_dict_pred)\n",
        "  ################################################################\n",
        "  ################################################################\n",
        "  ## By Reconstruction (Model)\n",
        "  print(\"By Reconstruction (Model)\")\n",
        "  y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train)\n",
        "  y_test_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_test_latent_pred, model_dec, X_test)\n",
        "\n",
        "  y_train_src_rec_pred = add_dimensions(y_train_src_rec_pred)\n",
        "  y_test_src_rec_pred = add_dimensions(y_test_src_rec_pred)\n",
        "\n",
        "  #y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train, argmax=True)\n",
        "  #y_test_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_test_latent_pred, model_dec, X_test, argmax=True)\n",
        "  print('y_train.shape:',y_train.shape,'y_train_src_rec_pred.shape',y_train_src_rec_pred.shape)\n",
        "  train_src_rec_acc = accuracy_score(y_train,y_train_src_rec_pred)\n",
        "  # train_src_rec_acc = accuracy_score(y_train.argmax(axis=1),y_train_src_rec_pred)\n",
        "  test_src_rec_acc = accuracy_score(y_test,y_test_src_rec_pred)\n",
        "  # test_src_rec_acc = accuracy_score(y_test.argmax(axis=1),y_test_src_rec_pred)\n",
        "\n",
        "  print(\"train_acc_by_Norm: {}, test_acc_by_Norm: {}\".format(train_src_pred_acc, test_src_pred_acc))\n",
        "  print(\"train_acc_by_Dict: {}, test_acc_by_Dict: {}\".format(train_src_dict_acc, test_src_dict_acc))\n",
        "  print(\"train_acc_by_Recon: {}, test_acc_by_Recon: {}\".format(train_src_rec_acc, test_src_rec_acc))\n",
        "\n",
        "  ## Input Train loss\n",
        "  MSE_X_train_pred = -10*np.log10(np.mean(np.square(X_train_pred - X_train)))\n",
        "  #MSE_X_val_pred = -10*np.log10(np.mean(np.square(X_val_pred - X_val)))\n",
        "  MSE_X_test_pred = -10*np.log10(np.mean(np.square(X_test_pred - X_test)))\n",
        "\n",
        "  ## Sample Label loss\n",
        "  MSE_y_latent_train_pred = -10*np.log10(np.mean(np.square(y_train_latent_pred - y_train_latent)))\n",
        "\n",
        "  print(MSE_X_train_pred, MSE_y_latent_train_pred)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  train_list = [y_train_src_pred, y_train_src_dict_pred, y_train_src_rec_pred]\n",
        "  test_list = [y_test_src_pred, y_test_src_dict_pred, y_test_src_rec_pred]\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  Stats=[]\n",
        "\n",
        "  for i in range(3):\n",
        "    y_train_pred = train_list[i]\n",
        "    y_test_pred = test_list[i]\n",
        "\n",
        "    ## Training Measures\n",
        "    tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi = Calculate_Stats(y_train, y_train_pred);\n",
        "\n",
        "    ## Validation Measures\n",
        "    #v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi = Calculate_Stats(to_categorical(y_val),y_val_pred);\n",
        "\n",
        "    ## Test Measures\n",
        "    t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi = Calculate_Stats(y_test,y_test_pred);\n",
        "\n",
        "    Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi,\n",
        "                  #              v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi,\n",
        "                  t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi])\n",
        "\n",
        "  Statistics = np.asarray(Stats)\n",
        "\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  Show_Statistics('Norm Training Results (MEAN)',Statistics[0][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "  Show_Statistics('Norm Test Results (MEAN)',Statistics[0][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "  Show_Statistics('Dict Training Results (MEAN)',Statistics[1][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "  Show_Statistics('Dict Test Results (MEAN)',Statistics[1][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "  Show_Statistics('Rec Training Results (MEAN)',Statistics[2][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "  Show_Statistics('Rec Test Results (MEAN)',Statistics[2][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "  #Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[14:21],Statistics.std(axis=0)[14:21], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before this it is working"
      ],
      "metadata": {
        "id": "85GMmSFHKYKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urNobQ7KuKF0"
      },
      "outputs": [],
      "source": [
        "# train_list = [y_train_src_pred, y_train_src_dict_pred, y_train_src_rec_pred]\n",
        "# test_list = [y_test_src_pred, y_test_src_dict_pred, y_test_src_rec_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3JRklKvsgnH"
      },
      "outputs": [],
      "source": [
        "# ################################################################################\n",
        "# ################################################################################\n",
        "# train_list = [y_train_src_pred, y_train_src_dict_pred, y_train_src_rec_pred]\n",
        "# test_list = [y_test_src_pred, y_test_src_dict_pred, y_test_src_rec_pred]\n",
        "# ################################################################################\n",
        "# ################################################################################\n",
        "# Stats=[]\n",
        "\n",
        "# for i in range(3):\n",
        "#   y_train_pred = train_list[i]\n",
        "#   y_test_pred = test_list[i]\n",
        "\n",
        "#   ## Training Measures\n",
        "#   tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi = Calculate_Stats(y_train, y_train_pred);\n",
        "\n",
        "#   ## Validation Measures\n",
        "#   #v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi = Calculate_Stats(to_categorical(y_val),y_val_pred);\n",
        "\n",
        "#   ## Test Measures\n",
        "#   t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi = Calculate_Stats(y_test,y_test_pred);\n",
        "\n",
        "#   Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi,\n",
        "#                 #              v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi,\n",
        "#                 t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi])\n",
        "\n",
        "# Statistics = np.asarray(Stats)\n",
        "\n",
        "# ################################################################################\n",
        "# ################################################################################\n",
        "# Show_Statistics('Norm Training Results (MEAN)',Statistics[0][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "# Show_Statistics('Norm Test Results (MEAN)',Statistics[0][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "# Show_Statistics('Dict Training Results (MEAN)',Statistics[1][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "# Show_Statistics('Dict Test Results (MEAN)',Statistics[1][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "# Show_Statistics('Rec Training Results (MEAN)',Statistics[2][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "# Show_Statistics('Rec Test Results (MEAN)',Statistics[2][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "# #Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[14:21],Statistics.std(axis=0)[14:21], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uvCv738thYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eae3c6a-1e99-4392-a2f3-c282a20a7d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NORM TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.234\n",
            "Sensitivity:1.0 + 0.469 \n",
            "Specificity:0.0 + 0.455\n",
            "F1-Score:0.749 + 0.416\n",
            "MCC:0.0 + 0.437\n",
            "Balance Accuracy:0.5 + 0.216\n",
            "Youden-Index:0.0 + 0.433\n",
            "----------------------------------------------------------------------\n",
            "NORM TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.599 + 0.186\n",
            "Sensitivity:1.0 + 0.463 \n",
            "Specificity:0.0 + 0.419\n",
            "F1-Score:0.749 + 0.39\n",
            "MCC:0.0 + 0.33\n",
            "Balance Accuracy:0.5 + 0.155\n",
            "Youden-Index:0.0 + 0.31\n",
            "----------------------------------------------------------------------\n",
            "DICT TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.965 + 0.234\n",
            "Sensitivity:0.99 + 0.469 \n",
            "Specificity:0.928 + 0.455\n",
            "F1-Score:0.971 + 0.416\n",
            "MCC:0.928 + 0.437\n",
            "Balance Accuracy:0.959 + 0.216\n",
            "Youden-Index:0.918 + 0.433\n",
            "----------------------------------------------------------------------\n",
            "DICT TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.855 + 0.186\n",
            "Sensitivity:0.961 + 0.463 \n",
            "Specificity:0.696 + 0.419\n",
            "F1-Score:0.888 + 0.39\n",
            "MCC:0.701 + 0.33\n",
            "Balance Accuracy:0.828 + 0.155\n",
            "Youden-Index:0.657 + 0.31\n",
            "----------------------------------------------------------------------\n",
            "REC TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.401 + 0.234\n",
            "Sensitivity:0.0 + 0.469 \n",
            "Specificity:1.0 + 0.455\n",
            "F1-Score:0.0 + 0.416\n",
            "MCC:0.0 + 0.437\n",
            "Balance Accuracy:0.5 + 0.216\n",
            "Youden-Index:0.0 + 0.433\n",
            "----------------------------------------------------------------------\n",
            "REC TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy:0.401 + 0.186\n",
            "Sensitivity:0.0 + 0.463 \n",
            "Specificity:1.0 + 0.419\n",
            "F1-Score:0.0 + 0.39\n",
            "MCC:0.0 + 0.33\n",
            "Balance Accuracy:0.5 + 0.155\n",
            "Youden-Index:0.0 + 0.31\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Show_Statistics('Norm Training Results (MEAN)',Statistics[0][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "# Show_Statistics('Norm Test Results (MEAN)',Statistics[0][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "# Show_Statistics('Dict Training Results (MEAN)',Statistics[1][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "# Show_Statistics('Dict Test Results (MEAN)',Statistics[1][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "# Show_Statistics('Rec Training Results (MEAN)',Statistics[2][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "# Show_Statistics('Rec Test Results (MEAN)',Statistics[2][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "# #Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[14:21],Statistics.std(axis=0)[14:21], 3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}