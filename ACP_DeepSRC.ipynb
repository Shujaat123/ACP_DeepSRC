{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/ACP_DeepSRC/blob/main/ACP_DeepSRC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-tcc6qiBvH3A"
      },
      "outputs": [],
      "source": [
        "import sys, os, re, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "\n",
        "## Models\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "from keras.utils import to_categorical # for keras > 2.0\n",
        "\n",
        "## Perfmetrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import auc, average_precision_score, precision_recall_curve, roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NO4GsEwi6UZp"
      },
      "outputs": [],
      "source": [
        "def SRC_Pred(y_latent_pred):\n",
        "    y_pred=[]\n",
        "    for i in range(y_latent_pred.shape[1]):\n",
        "\n",
        "        if (LA.norm(y_latent_pred[i][y_train==1])>= LA.norm(y_latent_pred[i][y_train==0])):\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "\n",
        "    return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ydW7vLSY4v6Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7d7VDajGydiX"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def Deep_SRC(input_shape=840, LV=600):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\n",
        "    enc_l1 = Dense(100, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\n",
        "\n",
        "    enc_l2 = Dense(100, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\n",
        "\n",
        "    enc_l3 = Dense(100, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\n",
        "\n",
        "    enc_l4 = Dense(100, activation='relu', name='encoder_layer4')(enc_l3)\n",
        "    enc_l4 = BatchNormalization()(enc_l4)\n",
        "    enc_l4 = Dropout(rate = 0.3)(enc_l4)\n",
        "\n",
        "    encoder_output = Dense(LV, activation='softmax', name='encoder_output')(enc_l4)\n",
        "\n",
        "    # # Classifier Network\n",
        "    #class_output = Dense(2, activation='softmax', name='class_layer1')(encoder_output)\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l4)\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[encoder_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\n",
        "                  loss={'encoder_output': 'categorical_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  loss_weights={'encoder_output': 0.001, 'decoder_output': 0.999},\n",
        "                  metrics=[metrics.categorical_accuracy])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ams3z_J54Dq1"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def Deep_VAE_SRC(input_shape =840, LV=600):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\n",
        "    enc_l1 = Dense(100, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\n",
        "\n",
        "    enc_l2 = Dense(100, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\n",
        "\n",
        "    enc_l3 = Dense(100, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\n",
        "\n",
        "    # enc_l4 = Dense(100, activation='relu', name='encoder_layer4')(enc_l3)\n",
        "    # enc_l4 = BatchNormalization()(enc_l4)\n",
        "    # enc_l4 = Dropout(rate = 0.3)(enc_l4)\n",
        "\n",
        "    #encoder_output = Dense(LV, activation='softmax', name='encoder_output')(enc_l4)\n",
        "    z_mean = Dense(LV, name=\"z_mean\")(enc_l3)\n",
        "    z_log_var = Dense(LV, name=\"z_log_var\")(enc_l3)\n",
        "    encoder_output = Sampling()([z_mean, z_log_var])\n",
        "    encoder_output = Dense(LV, activation='relu', name='encoder_output')(encoder_output)\n",
        "\n",
        "    # # Classifier Network\n",
        "    #class_output = Dense(2, activation='softmax', name='class_layer1')(encoder_output)\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l4)\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[encoder_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\n",
        "                  loss={'encoder_output': 'categorical_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  loss_weights={'encoder_output': 0.5, 'decoder_output': 0.5},\n",
        "                  metrics=[metrics.categorical_accuracy])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lmKm-WgAYTva"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def Deep_SRC_Decoder(input_shape =600):\n",
        "    # Encoder Network\n",
        "    dec_input = Input(shape=(input_shape,), name='decoder_input')\n",
        "\n",
        "    # Decoder Network\n",
        "    dec_l1 = Dense(100, activation='relu', name='decoder_layer1')(dec_input)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    dec_l2 = Dense(100, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(100, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    dec_l4 = Dense(100, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    dec_l4 = BatchNormalization()(dec_l4)\n",
        "    dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(840, activation='sigmoid', name='decoder_output')(dec_l4)\n",
        "\n",
        "    model = Model(inputs=[dec_input], outputs=[decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='adam',#optimizer='rmsprop',\n",
        "                  loss={'decoder_output': 'mean_squared_error'},\n",
        "                  metrics=[metrics.mean_squared_error])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M679bns3u7p7"
      },
      "outputs": [],
      "source": [
        "## Define CKSAAP feature-extraction function\n",
        "def minSequenceLength(fastas):\n",
        "\tminLen = 10000\n",
        "\tfor i in fastas:\n",
        "\t\tif minLen > len(i[1]):\n",
        "\t\t\tminLen = len(i[1])\n",
        "\treturn minLen\n",
        "\n",
        "def CKSAAP(fastas, gap=5, **kw):\n",
        "\tif gap < 0:\n",
        "\t\tprint('Error: the gap should be equal or greater than zero' + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tif minSequenceLength(fastas) < gap+2:\n",
        "\t\tprint('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tAA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\tencodings = []\n",
        "\taaPairs = []\n",
        "\tfor aa1 in AA:\n",
        "\t\tfor aa2 in AA:\n",
        "\t\t\taaPairs.append(aa1 + aa2)\n",
        "\theader = ['#']\n",
        "\tfor g in range(gap+1):\n",
        "\t\tfor aa in aaPairs:\n",
        "\t\t\theader.append(aa + '.gap' + str(g))\n",
        "\tencodings.append(header)\n",
        "\tfor i in fastas:\n",
        "\t\tname, sequence = i[0], i[1]\n",
        "\t\tcode = [name]\n",
        "\t\tfor g in range(gap+1):\n",
        "\t\t\tmyDict = {}\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tmyDict[pair] = 0\n",
        "\t\t\tsum = 0\n",
        "\t\t\tfor index1 in range(len(sequence)):\n",
        "\t\t\t\tindex2 = index1 + g + 1\n",
        "\t\t\t\tif index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\n",
        "\t\t\t\t\tmyDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
        "\t\t\t\t\tsum = sum + 1\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tcode.append(myDict[pair] / sum)\n",
        "\t\tencodings.append(code)\n",
        "\treturn encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2yZMgda6FGMm"
      },
      "outputs": [],
      "source": [
        "train_set = pd.read_csv(\"https://raw.githubusercontent.com/Shujaat123/AFP-SRC/master/data/train1.csv\")\n",
        "test_set = pd.read_csv(\"https://raw.githubusercontent.com/Shujaat123/AFP-SRC/master/data/test1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ua12za40GXE6"
      },
      "outputs": [],
      "source": [
        "# from keras.utils.np_utils import to_categorical\n",
        "X_train = train_set.iloc[:, 1:].to_numpy()\n",
        "y_train = np.asarray(train_set.CLASS)\n",
        "y_train[y_train=='AFP']=1\n",
        "y_train[y_train=='NON_AFP']=0\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "X_test = test_set.iloc[:, 1:].to_numpy()\n",
        "y_test = np.asarray(test_set.CLASS)\n",
        "y_test[y_test=='AFP']=1\n",
        "y_test[y_test=='NON_AFP']=0\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nLhyrZXbvitu"
      },
      "outputs": [],
      "source": [
        "def sample_one_hot_encoder(label):\n",
        "  ntrain = len(y_train)\n",
        "  onehot_encoded = list()\n",
        "  for value in range(ntrain):\n",
        "    letter = [0 for _ in range(ntrain)]\n",
        "    letter[value] = 1\n",
        "    onehot_encoded.append(letter)\n",
        "\n",
        "  return np.array(onehot_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HbJtcuKxABo",
        "outputId": "0cb18f62-5850-474f-e463-7b688e19c75f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y_train_latent = sample_one_hot_encoder(y_train)\n",
        "y_train_latent.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fkGc8BX_32Wq"
      },
      "outputs": [],
      "source": [
        "import numpy.linalg as LA\n",
        "def SRC_Pred(y_latent_pred):\n",
        "    y_pred=[]\n",
        "    for i in range(y_latent_pred.shape[0]):\n",
        "\n",
        "        if (LA.norm(y_latent_pred[i][y_train.argmax(axis=1)==1])>= LA.norm(y_latent_pred[i][y_train.argmax(axis=1)==0])):\n",
        "            y_pred.append(1)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "\n",
        "    return to_categorical(np.array(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwsXsaBcMBOz",
        "outputId": "5402a97e-489f-4e17-b0f0-e2eb18440732"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9372, 840)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YL2lNsS1zqmf"
      },
      "outputs": [],
      "source": [
        "# y_train_latent_pred.shape, y_test_latent_pred.shape\n",
        "# y_train_pred = SRC_Pred_Dict_Recon(y_train, y_train_latent_pred, X_train, X_train)\n",
        "# y_test_pred = SRC_Pred_Dict_Recon(y_test, y_test_latent_pred, X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEwgli58CaQr",
        "outputId": "bbe2d087-5680-48c3-c465-d44a58c4c8bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01275478, 0.03467109, 0.25618664, 0.69638749])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def custom_softmax(x):\n",
        "  exp_x = np.exp(x - np.max(x))\n",
        "  return exp_x / np.sum(exp_x)\n",
        "x = np.array([1, 2, 4, 5])\n",
        "custom_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7fjpqSTKGWBB"
      },
      "outputs": [],
      "source": [
        "def SRC_Pred_Dict_Recon(y_train, y_latent_pred, X_train, X_test):\n",
        "  npos = (y_train.argmax(axis=1)==1).sum()\n",
        "  nneg = len(y_train) - npos\n",
        "  y_latent_pred_pos = []\n",
        "  y_latent_pred_neg = []\n",
        "  for i in range(y_latent_pred.shape[0]):\n",
        "     y_latent_pred_pos.append(np.concatenate((np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1], np.repeat(0,nneg)), axis=0))\n",
        "     y_latent_pred_neg.append(np.concatenate((np.repeat(0,npos), np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1]), axis=0))\n",
        "\n",
        "  X_pred_pos = np.matmul(np.array(y_latent_pred_pos), X_train)\n",
        "  X_pred_neg = np.matmul(np.array(y_latent_pred_neg), X_train)\n",
        "\n",
        "  y_pred=[]\n",
        "  for i in range(X_test.shape[0]):\n",
        "    pos_err = np.square(X_test[i] - X_pred_pos).sum()\n",
        "    neg_err = np.square(X_test[i] - X_pred_neg).sum()\n",
        "    if pos_err <= neg_err:\n",
        "      y_pred.append(1)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "\n",
        "  return to_categorical(np.array(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fkZEW7TIJ_QM"
      },
      "outputs": [],
      "source": [
        "def SRC_Pred_Dec_Recon(y_train, y_latent_pred, model, X_test, argmax=False):\n",
        "  npos = (y_train.argmax(axis=1)==1).sum()\n",
        "  nneg = len(y_train) - npos\n",
        "  y_latent_pred_pos = []\n",
        "  y_latent_pred_neg = []\n",
        "  for i in range(y_latent_pred.shape[0]):\n",
        "    if argmax:\n",
        "      tmp_y_latent_pred_pos = np.repeat(0, len(y_latent_pred[i]))\n",
        "      tmp_y_latent_pred_pos[y_latent_pred[i][y_train==1].argmax(axis=1)] = 1\n",
        "      tmp_y_latent_pred_neg = np.repeat(0, len(y_latent_pred[i]))\n",
        "      tmp_y_latent_pred_neg[y_latent_pred[i][y_train==0].argmax(axis=1)] = 1\n",
        "      y_latent_pred_pos.append(tmp_y_latent_pred_pos)\n",
        "      y_latent_pred_neg.append(tmp_y_latent_pred_neg)\n",
        "    else:\n",
        "     y_latent_pred_pos.append(np.concatenate((np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==1], np.repeat(0,nneg)), axis=0))\n",
        "     y_latent_pred_neg.append(np.concatenate((np.repeat(0,npos), np.array(custom_softmax(y_latent_pred[i]))[y_train.argmax(axis=1)==0]), axis=0))\n",
        "\n",
        "  y_latent_pred_pos = np.array(y_latent_pred_pos)\n",
        "  y_latent_pred_neg = np.array(y_latent_pred_neg)\n",
        "  X_pred_pos = model_dec.predict(y_latent_pred_pos)\n",
        "  X_pred_neg = model_dec.predict(y_latent_pred_neg)\n",
        "\n",
        "  y_pred=[]\n",
        "  for i in range(X_test.shape[0]):\n",
        "    pos_err = np.square(X_test[i] - X_pred_pos[i]).sum()\n",
        "    neg_err = np.square(X_test[i] - X_pred_neg[i]).sum()\n",
        "    if pos_err <= neg_err:\n",
        "      y_pred.append(1)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "\n",
        "  return to_categorical(np.array(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6sZOgQnV2Mfx"
      },
      "outputs": [],
      "source": [
        "y_latent_pred = np.random.rand(len(y_train),600)\n",
        "y_pred = SRC_Pred(y_latent_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xeh-L3Jp7icZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bJ6O1q4Y8VFh"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwjTfOFszbhv",
        "outputId": "24261989-9d8b-4aa9-f53e-107a863da58e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model = Deep_SRC(input_shape = X_train.shape[1], LV=len(y_train))\n",
        "#model = Deep_VAE_SRC(input_shape = X_train.shape[1], LV=len(y_train))\n",
        "es = EarlyStopping(monitor='encoder_output_categorical_accuracy', mode='max', verbose=0, patience=100)\n",
        "checkpoint = ModelCheckpoint('models\\\\model-best.h5',\n",
        "                             verbose=0, monitor='encoder_output_categorical_accuracy',save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit({'enc_input': X_train},\n",
        "                    {'encoder_output': y_train_latent, 'decoder_output': X_train},\n",
        "                    # validation_data = ({'enc_input': X_val},\n",
        "                    # {'class_output': y_val, 'decoder_output': X_val}),\n",
        "                    epochs=5000, batch_size=420, callbacks=[checkpoint, es], verbose=0)\n",
        "\n",
        "y_train_latent_pred, X_train_pred = model.predict(X_train,batch_size=540, verbose=0)\n",
        "#y_val_latent_pred, X_val_pred = model.predict(X_val,batch_size=540, verbose=0)\n",
        "y_test_latent_pred, X_test_pred = model.predict(X_test,batch_size=540, verbose=0)\n",
        "\n",
        "#del model\n",
        "#model = load_model('models\\\\model-best.h5')\n",
        "\n",
        "model_dec = Deep_SRC_Decoder()\n",
        "\n",
        "# es = EarlyStopping(monitor='loss', mode='min', verbose=0, patience=100)\n",
        "# checkpoint = ModelCheckpoint('models\\\\model_dec-best.h5',\n",
        "#                              verbose=0, monitor='loss',save_best_only=True, mode='auto')\n",
        "\n",
        "# history = model_dec.fit({'decoder_input': y_train_latent},\n",
        "#                     {'decoder_output': X_train},\n",
        "#                     # validation_data = ({'enc_input': X_val},\n",
        "#                     # {'class_output': y_val, 'decoder_output': X_val}),\n",
        "#                     epochs=5000, batch_size=600, callbacks=[checkpoint, es], verbose=0)\n",
        "# del model_dec\n",
        "# model_dec = load_model('models\\\\model_dec-best.h5')\n",
        "\n",
        "for i in range(1,12):\n",
        "    model_dec.layers[i].set_weights(model.layers[13+i].get_weights())\n",
        "\n",
        "## By Norm\n",
        "y_train_src_pred = SRC_Pred(y_train_latent_pred)\n",
        "y_test_src_pred = SRC_Pred(y_test_latent_pred)\n",
        "train_src_pred_acc = accuracy_score(y_train, y_train_src_pred)\n",
        "test_src_pred_acc = accuracy_score(y_test, y_test_src_pred)\n",
        "\n",
        "## By Dictionary (Train Dataset)\n",
        "y_train_src_dict_pred = SRC_Pred_Dict_Recon(y_train, y_train_latent_pred, X_train, X_train)\n",
        "y_test_src_dict_pred = SRC_Pred_Dict_Recon(y_train, y_test_latent_pred, X_train, X_test)\n",
        "train_src_dict_acc = accuracy_score(y_train, y_train_src_dict_pred)\n",
        "test_src_dict_acc = accuracy_score(y_test, y_test_src_dict_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQFmUIUPIap3"
      },
      "outputs": [],
      "source": [
        "## By Reconstruction (Model)\n",
        "y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train)\n",
        "y_test_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_test_latent_pred, model_dec, X_test)\n",
        "#y_train_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_train_latent_pred, model_dec, X_train, argmax=True)\n",
        "#y_test_src_rec_pred = SRC_Pred_Dec_Recon(y_train, y_test_latent_pred, model_dec, X_test, argmax=True)\n",
        "train_src_rec_acc = accuracy_score(y_train,y_train_src_rec_pred)\n",
        "test_src_rec_acc = accuracy_score(y_test,y_test_src_rec_pred)\n",
        "\n",
        "print(\"train_acc_by_Norm: {}, test_acc_by_Norm: {}\".format(train_src_pred_acc, test_src_pred_acc))\n",
        "print(\"train_acc_by_Dict: {}, test_acc_by_Dict: {}\".format(train_src_dict_acc, test_src_dict_acc))\n",
        "print(\"train_acc_by_Recon: {}, test_acc_by_Recon: {}\".format(train_src_rec_acc, test_src_rec_acc))\n",
        "\n",
        "## Input Train loss\n",
        "MSE_X_train_pred = -10*np.log10(np.mean(np.square(X_train_pred - X_train)))\n",
        "#MSE_X_val_pred = -10*np.log10(np.mean(np.square(X_val_pred - X_val)))\n",
        "MSE_X_test_pred = -10*np.log10(np.mean(np.square(X_test_pred - X_test)))\n",
        "\n",
        "## Sample Label loss\n",
        "MSE_y_latent_train_pred = -10*np.log10(np.mean(np.square(y_train_latent_pred - y_train_latent)))\n",
        "\n",
        "print(MSE_X_train_pred, MSE_y_latent_train_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz3Q97vju6Ho"
      },
      "outputs": [],
      "source": [
        "def yoden_index(y, y_pred):\n",
        "  epsilon = 1e-30\n",
        "  tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
        "  j = (tp/(tp + fn + epsilon)) + (tn/(tn+fp + epsilon)) - 1\n",
        "  return j\n",
        "\n",
        "def pmeasure(y, y_pred):\n",
        "    epsilon = 1e-30\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
        "    sensitivity = tp / (tp + fn + epsilon)\n",
        "    specificity = tn / (tn + fp + epsilon)\n",
        "    f1score = (2 * tp) / (2 * tp + fp + fn + epsilon)\n",
        "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDk5IAHJs3f4"
      },
      "outputs": [],
      "source": [
        "def Calculate_Stats(y_actual,y_pred):\n",
        "  acc = accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  sen = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Sensitivity']\n",
        "  spe = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Specificity']\n",
        "  f1 = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['F1-Score']\n",
        "  mcc = matthews_corrcoef(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  bacc = balanced_accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  yi = yoden_index(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  #auc = roc_auc_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "\n",
        "  #pre, rec, _ = precision_recall_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\n",
        "  #fpr, tpr, _ = roc_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\n",
        "  #auroc = auc(fpr, tpr)\n",
        "  #aupr = auc(rec, pre)\n",
        "\n",
        "  return acc, sen, spe, f1, mcc, bacc, yi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urNobQ7KuKF0"
      },
      "outputs": [],
      "source": [
        "train_list = [y_train_src_pred, y_train_src_dict_pred, y_train_src_rec_pred]\n",
        "test_list = [y_test_src_pred, y_test_src_dict_pred, y_test_src_rec_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3JRklKvsgnH"
      },
      "outputs": [],
      "source": [
        "Stats=[]\n",
        "\n",
        "for i in range(3):\n",
        "  y_train_pred = train_list[i]\n",
        "  y_test_pred = test_list[i]\n",
        "\n",
        "  ## Training Measures\n",
        "  tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi = Calculate_Stats(y_train, y_train_pred);\n",
        "\n",
        "  ## Validation Measures\n",
        "  #v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi = Calculate_Stats(to_categorical(y_val),y_val_pred);\n",
        "\n",
        "  ## Test Measures\n",
        "  t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi = Calculate_Stats(y_test,y_test_pred);\n",
        "\n",
        "  Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi,\n",
        "                #              v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi,\n",
        "                t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi])\n",
        "\n",
        "Statistics = np.asarray(Stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaCNG--RtbIx"
      },
      "outputs": [],
      "source": [
        "def Show_Statistics(msg,mean_Stats, sd_Stats, sigfig):\n",
        "  print(msg.upper())\n",
        "  print(70*'-')\n",
        "  print('Accuracy:{} + {}'          .format(round(mean_Stats[0],sigfig), round(sd_Stats[0],sigfig)))\n",
        "  print('Sensitivity:{} + {} '      .format(round(mean_Stats[1],sigfig), round(sd_Stats[1],sigfig)))\n",
        "  print('Specificity:{} + {}'       .format(round(mean_Stats[2],sigfig), round(sd_Stats[2],sigfig)))\n",
        "  print('F1-Score:{} + {}'          .format(round(mean_Stats[3],sigfig), round(sd_Stats[3],sigfig)))\n",
        "  print('MCC:{} + {}'               .format(round(mean_Stats[4],sigfig), round(sd_Stats[4],sigfig)))\n",
        "  print('Balance Accuracy:{} + {}'  .format(round(mean_Stats[5],sigfig), round(sd_Stats[5],sigfig)))\n",
        "  print('Youden-Index:{} + {}'      .format(round(mean_Stats[6],sigfig), round(sd_Stats[6],sigfig)))\n",
        "  print(70*'-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uvCv738thYf"
      },
      "outputs": [],
      "source": [
        "Show_Statistics('Norm Training Results (MEAN)',Statistics[0][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "Show_Statistics('Norm Test Results (MEAN)',Statistics[0][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "Show_Statistics('Dict Training Results (MEAN)',Statistics[1][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "Show_Statistics('Dict Test Results (MEAN)',Statistics[1][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "Show_Statistics('Rec Training Results (MEAN)',Statistics[2][0:7],Statistics.std(axis=0)[0:7], 3)\n",
        "Show_Statistics('Rec Test Results (MEAN)',Statistics[2][7:14],Statistics.std(axis=0)[7:14], 3)\n",
        "#Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[14:21],Statistics.std(axis=0)[14:21], 3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}